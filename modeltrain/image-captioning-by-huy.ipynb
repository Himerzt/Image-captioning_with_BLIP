{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìå Sinh Caption cho ·∫¢nh v·ªõi BLIP v√† B·ªô D·ªØ li·ªáu COCO 2017\n\n## üéØ Gi·ªõi thi·ªáu\nƒê·ªÅ t√†i n√†y t·∫≠p trung v√†o **b√†i to√°n Image Captioning** ‚Äì t·∫°o ra m√¥ t·∫£ b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n cho m·ªôt b·ª©c ·∫£nh.  \nC·ª• th·ªÉ, notebook n√†y s·ª≠ d·ª•ng:\n- **BLIP (Bootstrapping Language-Image Pretraining)** ‚Äì m√¥ h√¨nh ti√™n ti·∫øn c·ªßa HuggingFace cho nhi·ªám v·ª• sinh caption t·ª´ ·∫£nh.\n- **B·ªô d·ªØ li·ªáu COCO 2017** ‚Äì b·ªô d·ªØ li·ªáu chu·∫©n trong th·ªã gi√°c m√°y t√≠nh, bao g·ªìm h√†ng trƒÉm ngh√¨n ·∫£nh k√®m nhi·ªÅu ch√∫ th√≠ch.\n\n## üèó Quy tr√¨nh th·ª±c hi·ªán\n1Ô∏è‚É£ **Chu·∫©n b·ªã m√¥i tr∆∞·ªùng & th∆∞ vi·ªán**: C√†i ƒë·∫∑t v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (PyTorch, Transformers, PIL‚Ä¶).  \n2Ô∏è‚É£ **Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu COCO**: ƒê·ªçc annotation, √°nh x·∫° ·∫£nh v·ªõi caption t∆∞∆°ng ·ª©ng.  \n3Ô∏è‚É£ **X√¢y d·ª±ng Dataset t√πy ch·ªânh**: T·∫°o class `COCODirectDataset` ƒë·ªÉ k·∫øt h·ª£p ·∫£nh v√† caption qua `BlipProcessor`.  \n4Ô∏è‚É£ **T√≠ch h·ª£p m√¥ h√¨nh BLIP**: S·ª≠ d·ª•ng model `Salesforce/blip-image-captioning-base` t·ª´ HuggingFace.  \n5Ô∏è‚É£ **Sinh caption v√† minh h·ªça k·∫øt qu·∫£**: Cho ·∫£nh v√†o model ƒë·ªÉ t·∫°o caption, hi·ªÉn th·ªã v√† ƒë√°nh gi√°.\n\n## ‚úÖ K·∫øt qu·∫£ k·ª≥ v·ªçng\nNotebook n√†y s·∫Ω cho th·∫•y **c√°ch x√¢y d·ª±ng pipeline sinh caption ho√†n ch·ªânh** t·ª´ d·ªØ li·ªáu th√¥ ƒë·∫øn k·∫øt qu·∫£ tr·ª±c quan, ƒë·ªìng th·ªùi t·∫°o n·ªÅn t·∫£ng cho vi·ªác **fine-tuning** ho·∫∑c tri·ªÉn khai v√†o ·ª©ng d·ª•ng th·ª±c t·∫ø (nh∆∞ tr·ª£ l√Ω AI cho ng∆∞·ªùi khi·∫øm th·ªã ho·∫∑c qu·∫£n l√Ω ·∫£nh th√¥ng minh).\n","metadata":{}},{"cell_type":"code","source":"print(\"Nguyen Quoc Huy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T10:42:56.234483Z","iopub.execute_input":"2025-08-03T10:42:56.235103Z","iopub.status.idle":"2025-08-03T10:42:56.242965Z","shell.execute_reply.started":"2025-08-03T10:42:56.235077Z","shell.execute_reply":"2025-08-03T10:42:56.242139Z"}},"outputs":[{"name":"stdout","text":"Nguyen Quoc Huy\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"1Ô∏è‚É£ Chu·∫©n b·ªã m√¥i tr∆∞·ªùng & th∆∞ vi·ªán: C√†i ƒë·∫∑t v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (PyTorch, Transformers, PIL‚Ä¶).","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.optim import AdamW\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BlipProcessor, BlipForConditionalGeneration, get_scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T10:42:56.244163Z","iopub.execute_input":"2025-08-03T10:42:56.244508Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2Ô∏è‚É£ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu COCO: ƒê·ªçc annotation, √°nh x·∫° ·∫£nh v·ªõi caption t∆∞∆°ng ·ª©ng.","metadata":{}},{"cell_type":"code","source":"ANNOTATION_PATH = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json\"\n\nwith open(ANNOTATION_PATH, 'r') as f:\n    coco_data = json.load(f)\n\ncaption_count = defaultdict(int)\nfor ann in coco_data['annotations']:\n    caption_count[ann['image_id']] += 1\n\nall_counts = list(caption_count.values())\n\nprint(f\"S·ªë l∆∞·ª£ng ·∫£nh: {len(caption_count)}\")\nprint(f\"Caption m·ªói ·∫£nh (trung b√¨nh): {sum(all_counts)/len(all_counts):.2f}\")\nprint(f\"Th·ªëng k√™ chi ti·∫øt:\")\nprint(Counter(all_counts))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ANNOTATION_PATH = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json\"\nIMG_DIR = \"/kaggle/input/coco-2017-dataset/coco2017/train2017\"\n\nwith open(ANNOTATION_PATH, 'r') as f:\n    coco_data = json.load(f)\n\nid_to_info = {img['id']: img for img in coco_data['images']}\n\nimport random\nN = 3\nsamples = random.sample(coco_data['annotations'], N)\n\nfor ann in samples:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    \n    img_info = id_to_info.get(image_id, None)\n    if img_info is None:\n        print(f\" Kh√¥ng t√¨m th·∫•y ·∫£nh v·ªõi id: {image_id}\")\n        continue\n    \n    file_name = img_info['file_name']\n    file_path = os.path.join(IMG_DIR, file_name)\n\n    print(f\"\\n ·∫¢nh: {file_name}\")\n    print(f\" K√≠ch th∆∞·ªõc: {img_info['width']}x{img_info['height']}\")\n    print(f\" Caption: {caption}\")\n\n    if os.path.exists(file_path):\n        try:\n            image = Image.open(file_path)\n            plt.imshow(image)\n            plt.axis(\"off\")\n            plt.title(caption)\n            plt.show()\n        except Exception as e:\n            print(f\" L·ªói khi m·ªü ·∫£nh: {e}\")\n    else:\n        print(\" ·∫¢nh kh√¥ng t·ªìn t·∫°i trong th∆∞ m·ª•c.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt cho m√¥ h√¨nh","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ki·ªÉm tra GPU","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\" D√πng thi·∫øt b·ªã:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ƒê·∫£m b·∫£o d·ªØ li·ªáu train s·∫µn s√†ng.","metadata":{}},{"cell_type":"code","source":"COCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\nTRAIN_IMG_DIR = f\"{COCO_ROOT}/train2017\"\nANNOTATION_PATH = f\"{COCO_ROOT}/annotations/captions_train2017.json\"\n\nassert os.path.exists(TRAIN_IMG_DIR), \" Th∆∞ m·ª•c ·∫£nh train kh√¥ng t·ªìn t·∫°i\"\nassert os.path.exists(ANNOTATION_PATH), \" File annotation train kh√¥ng t·ªìn t·∫°i\"\n\nwith open(ANNOTATION_PATH, 'r') as f:\n    coco_data = json.load(f)\nprint(\" ƒê√£ load annotation train JSON\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chu·∫©n b·ªã d·ªØ li·ªáu cho Dataset","metadata":{}},{"cell_type":"code","source":"# B∆∞·ªõc 1: √°nh x·∫° image_id -> file_name\nid_to_filename = {img['id']: img['file_name'] for img in coco_data['images']}\n\n# B∆∞·ªõc 2: √°nh x·∫° image_id -> danh s√°ch caption\nimage_captions = defaultdict(list)\nfor ann in coco_data['annotations']:\n    img_id = ann['image_id']\n    caption = ann['caption']\n    image_captions[img_id].append(caption)\n\n# B∆∞·ªõc 3: l·ªçc c√°c ·∫£nh t·ªìn t·∫°i th·ª±c s·ª±\nvalid_image_ids = []\nfor img_id, file_name in id_to_filename.items():\n    img_path = os.path.join(TRAIN_IMG_DIR, file_name)\n    if os.path.exists(img_path) and img_id in image_captions:\n        valid_image_ids.append(img_id)\n\n# B∆∞·ªõc 4: sort image_id ƒë·ªÉ ·ªïn ƒë·ªãnh, skip 50k v√† l·∫•y 30k ·∫£nh v√¨ gi·ªõi h·∫°n CPU\nvalid_image_ids = sorted(valid_image_ids)\nselected_ids = valid_image_ids[50000:80000]  \n\n# B∆∞·ªõc 5: t·∫°o annotation final (m·ªói ·∫£nh l·∫•y t·ªëi ƒëa 2 caption) l·∫•y 2 caption ƒë·ªÉ ƒëa d·∫°ng h√≥a caption cho ·∫£nh\nannotations = []\nfor img_id in selected_ids:\n    file_name = id_to_filename[img_id]\n    captions = image_captions[img_id][:2]  \n    for cap in captions:\n        annotations.append({\"image\": file_name, \"caption\": cap})\n\nprint(f\"‚úÖ ƒê√£ ch·ªçn {len(selected_ids)} ·∫£nh (skip 50k ƒë·∫ßu), t·ªïng {len(annotations)} caption (m·ªói ·∫£nh t·ªëi ƒëa 2).\")\n\n# Ki·ªÉm tra ph√¢n ph·ªëi caption\ntrain_caption_per_image = defaultdict(int)\nfor item in annotations:\n    train_caption_per_image[item[\"image\"]] += 1\n\nprint(f\"üßæ Train: {len(annotations)} caption tr√™n {len(train_caption_per_image)} ·∫£nh\")\nprint(f\"üìà Ph√¢n ph·ªëi caption/·∫£nh (train): {dict(Counter(train_caption_per_image.values()))}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"X√¢y d·ª±ng Dataset custom ƒë·ªÉ model d√πng tr·ª±c ti·∫øp","metadata":{}},{"cell_type":"code","source":"class COCODirectDataset(Dataset):\n    def __init__(self, annotations, img_dir, processor, max_length=128):\n        self.data = annotations\n        self.img_dir = img_dir\n        self.processor = processor\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        img_path = os.path.join(self.img_dir, item[\"image\"])\n        caption = item[\"caption\"]\n\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\" L·ªói khi m·ªü ·∫£nh: {img_path} ‚Äî {e}\")\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n\n        inputs = self.processor(images=image, text=caption, padding=\"max_length\",\n                                truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        return inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chu·∫©n b·ªã d·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho training.","metadata":{}},{"cell_type":"code","source":"processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = COCODirectDataset(annotations, TRAIN_IMG_DIR, processor)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nprint(f\" DataLoader train ‚Äî {len(train_dataset)} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"L·∫•y BLIP model pretrained, ƒë√≥ng bƒÉng ph·∫ßn x·ª≠ l√Ω ·∫£nh (vision encoder) ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n","metadata":{}},{"cell_type":"code","source":"model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\nfor param in model.vision_model.parameters():\n    param.requires_grad = False\n\nprint(f\" Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\nprint(f\" Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f}M\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chu·∫©n b·ªã d·ªØ li·ªáu validation ƒë·ªÉ ƒë√°nh gi√° model.","metadata":{}},{"cell_type":"code","source":"VAL_IMG_DIR = f\"{COCO_ROOT}/val2017\"\nVAL_ANN_PATH = f\"{COCO_ROOT}/annotations/captions_val2017.json\"\n\nassert os.path.exists(VAL_IMG_DIR), \" Th∆∞ m·ª•c ·∫£nh val kh√¥ng t·ªìn t·∫°i\"\nassert os.path.exists(VAL_ANN_PATH), \" File annotation val kh√¥ng t·ªìn t·∫°i\"\n\nwith open(VAL_ANN_PATH, \"r\") as f:\n    val_data = json.load(f)\n\nval_id_to_file = {img[\"id\"]: img[\"file_name\"] for img in val_data[\"images\"]}\nval_annotations = []\nfor ann in val_data[\"annotations\"]:\n    img_id = ann[\"image_id\"]\n    if img_id in val_id_to_file:\n        file_name = val_id_to_file[img_id]\n        img_path = os.path.join(VAL_IMG_DIR, file_name)\n        if os.path.exists(img_path):\n            val_annotations.append({\"image\": file_name, \"caption\": ann[\"caption\"]})\n\nrandom.seed(123)\nval_annotations = random.sample(val_annotations, 5000)\nprint(f\"‚úÖ S·ªë sample validation: {len(val_annotations)}\")\n\nval_caption_per_image = defaultdict(int)\nfor item in val_annotations:\n    val_caption_per_image[item[\"image\"]] += 1\nprint(f\"üßæ Validation: {len(val_annotations)} caption tr√™n {len(val_caption_per_image)} ·∫£nh\")\nprint(f\"üìà Ph√¢n ph·ªëi caption/·∫£nh (val): {dict(Counter(val_caption_per_image.values()))}\")\n\ntrain_images = set(item[\"image\"] for item in annotations)\nval_images = set(item[\"image\"] for item in val_annotations)\noverlap = train_images.intersection(val_images)\nprint(f\"‚ö†Ô∏è S·ªë ·∫£nh tr√πng gi·ªØa train v√† val: {len(overlap)}\")\n\nval_dataset = COCODirectDataset(val_annotations, VAL_IMG_DIR, processor)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\nprint(f\"‚úÖ DataLoader valid ‚Äî {len(val_dataset)} samples\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chu·∫©n b·ªã c∆° ch·∫ø t·ªëi ∆∞u cho training.\n\n","metadata":{}},{"cell_type":"code","source":"def setup_optimizer_scheduler(model, train_loader, lr=5e-5, epochs=3):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    total_steps = len(train_loader) * epochs\n    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n    print(f\" AdamW + Scheduler  ‚Äî Total steps: {total_steps}\")\n    return optimizer, scheduler\n\noptimizer, scheduler = setup_optimizer_scheduler(model, train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, train_loader, val_loader, optimizer, scheduler, device, epochs=3):\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"üöÄ Epoch {epoch+1}/{epochs}\")\n\n        for batch in pbar:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch, labels=batch[\"input_ids\"])\n            loss = outputs.loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            total_loss += loss.item()\n            pbar.set_postfix(train_loss=loss.item())\n\n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"‚úÖ Epoch {epoch+1} ‚Äî Train loss: {avg_train_loss:.4f}\")\n\n        evaluate(model, val_loader, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(model, train_loader, val_loader, optimizer, scheduler, device, epochs=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch, labels=batch[\"input_ids\"])\n            total_loss += outputs.loss.item()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\" Validation Loss: {avg_loss:.4f}\")\n    model.train()\n    return avg_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(model, processor, image_path, device, max_length=32):\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n    model.eval()\n    with torch.no_grad():\n        output_ids = model.generate(**inputs, max_length=max_length)\n        caption = processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return caption\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preview_val_predictions(model, processor, val_data, img_dir, device, num_samples=5):\n    samples = random.sample(val_data, num_samples)\n    for sample in samples:\n        img_path = os.path.join(img_dir, sample[\"image\"])\n        gt_caption = sample[\"caption\"]\n        try:\n            pred_caption = generate_caption(model, processor, img_path, device)\n            image = Image.open(img_path).convert(\"RGB\")\n            plt.imshow(image)\n            plt.axis(\"off\")\n            plt.title(f\"üìå Pred: {pred_caption}\\n‚úÖ GT: {gt_caption}\", fontsize=9)\n            plt.show()\n        except Exception as e:\n            print(f\"‚ùå L·ªói ·∫£nh {img_path}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preview_val_predictions(model, processor, val_annotations, VAL_IMG_DIR, device, num_samples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_DIR = \"./blip-captioning-final\"\n\nmodel.save_pretrained(OUTPUT_DIR)\nprocessor.save_pretrained(OUTPUT_DIR)\n\nprint(f\"‚úÖ M√¥ h√¨nh v√† processor ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}